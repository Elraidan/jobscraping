from bs4 import BeautifulSoup
import requests
import re
import pandas as pd
from time import sleep

# Define the URL to scrape
URL = 'https://www.monster.fi/tyopaikat/haku/?q=data-analytics&where=Helsinki'

# Send a GET request to the website
page = requests.get(URL)
soup = BeautifulSoup(page.text, 'html.parser')

# Try to find the element with the number of jobs
num_jobs_element = soup.find('h2', attrs={'class': 'figure'})

# If the element is found, extract the number of jobs
if num_jobs_element:
    num = sum(int(i) for i in re.findall(r'\d+', num_jobs_element.text))
    print(f'There are {num} jobs found')
else:
    print("Number of jobs not found on the page")

# Extracting job data using find_all instead of the deprecated findAll
companies = [company.text.strip() for company in soup.find_all(name='div', attrs={'class': 'company'})]
locations = [location.text.strip() for location in soup.find_all(name='div', attrs={'class': 'location'})]
titles = [title.text.strip() for title in soup.find_all(name='h2', attrs={'class': 'title'})]
links = [a.find('a').attrs['href'] for a in soup.find_all(name='h2', attrs={'class': 'title'})]

# Print extracted data for debugging
print(companies)
print(locations)
print(titles)
print(links)

# Combine the extracted data into a list of dictionaries
job_data = []
for title, company, location, link in zip(titles, companies, locations, links):
    job_data.append({'Title': title, 'Company': company, 'Location': location, 'Link': link})

# Create a DataFrame from the job data
df = pd.DataFrame(job_data)

# Define the save path for the Excel file
save_path = r"C:\Users\Niklas\Documents\Python\Job_Listings.xlsx"

# Check if we have any data to save
if not df.empty:
    try:
        # Save the DataFrame to an Excel file
        df.to_excel(save_path, index=False)
        print(f"Excel file created successfully at {save_path}")
    except Exception as e:
        print(f"Error saving Excel file: {e}")
else:
    print("No job data to save.")
