from bs4 import BeautifulSoup
import requests
import re
from time import sleep

URL = 'https://www.monster.fi/tyopaikat/haku/?q=data-analytics&where=Helsinki'
page = requests.get(URL)
soup = BeautifulSoup(page.text, 'html.parser')
print(soup.prettify())

# Correcting the issue with the regular expression
num = sum(int(i) for i in re.findall(r'\d+', soup.find('h2', attrs={'class': 'figure'}).text))
print('There are ' + str(num) + ' jobs found')

companies = []
company = soup.findAll(name='div', attrs={'class': 'company'})
if len(company) > 0:
    for b in company:
        companies.append(b.text.strip())
print(companies)

locations = []
location = soup.findAll(name='div', attrs={'class': 'location'})
if len(location) > 0:
    for b in location:
        locations.append(b.text.strip())
print(locations)

titles = []
title = soup.findAll(name='h2', attrs={'class': 'title'})
if len(title) > 0:
    for b in title:
        titles.append(b.text.strip())
print(titles)

links = []
for a in title:
    url = a.find('a').attrs['href']
    links.append(url)
print(links)

# Fetch the job details from the specific job page
job_page = requests.get('https://avoimettyopaikat.monster.fi/osa-aikainen-data-scientist-data-analyst-houston-analytics-helsinki-uusi-finland-academic-work/11/199503456')
job_soup = BeautifulSoup(job_page.text, 'lxml')
job_desc = job_soup.find('div', attrs={'id': 'JobDescription'}).text
print(job_desc)

def clean_text(text):
    lines = (line.strip() for line in text.splitlines())  # break into lines
    
    chunks = (phrase.strip() for line in lines for phrase in line.split("  "))  # break multi-headlines into a line each
    
    def chunk_space(chunk):
        chunk_out = chunk + ' '  # Need to fix spacing issue
        return chunk_out
    
    def remove_html_tags(text):
        """Remove HTML tags from a string"""
        import re
        clean = re.compile('<.*?>')
        return re.sub(clean, '', text)
    
    text = remove_html_tags(text)

    text = ''.join(chunk_space(chunk) for chunk in chunks if chunk).encode('utf-8')  # Get rid of all blank lines and ends of line

    # Now clean out all of the unicode junk (this line works great!!!)
    try:
        text = text.decode('unicode_escape').encode('ascii', 'ignore')  # Need this as some websites aren't formatted
    except:  # in a way that this works, can occasionally throw an exception
        return  # Handle cases where the text cannot be decoded properly
    return text
