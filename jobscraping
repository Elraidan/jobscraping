from bs4 import BeautifulSoup
import requests
import pandas as pd
import re

# Define the URL to scrape
URL = 'https://www.jobly.fi/tyopaikat/tekniikka/uusimaa/kokopaivainen'

# Set headers to mimic a real browser request
HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/110.0.0.0 Safari/537.36"
}

# Send a GET request to the website
response = requests.get(URL, headers=HEADERS)

# Check if the request was successful
if response.status_code != 200:
    print(f"Failed to retrieve the page. Status Code: {response.status_code}")
    exit()

# Parse the HTML content
soup = BeautifulSoup(response.text, 'html.parser')

# Extract job details
jobs = []
for job_element in soup.find_all('h2'):
    title_elem = job_element.find('a', class_=re.compile(r'recruiter-job-link', re.I))
    location_elem = job_element.find_next('div', class_=re.compile(r'location', re.I))
    terms_elem = job_element.find_next('div', class_=re.compile(r'terms', re.I))
    
    title = title_elem.text.strip() if title_elem else 'N/A'
    link = title_elem['href'] if title_elem and 'href' in title_elem.attrs else 'N/A'
    location = location_elem.text.strip() if location_elem else 'N/A'
    terms = terms_elem.text.strip() if terms_elem else 'N/A'
    
    jobs.append({'Title': title, 'Location': location, 'Terms': terms, 'Link': link})

# Create a DataFrame from the job data
df = pd.DataFrame(jobs)

# Define the save path for the Excel file
save_path = r"C:\Users\Niklas\Documents\Python\Job_Listings.xlsx"

# Check if we have any data to save
if not df.empty:
    try:
        df.to_excel(save_path, index=False)
        print(f"Excel file created successfully at {save_path}")
    except Exception as e:
        print(f"Error saving Excel file: {e}")
else:
    print("No job data to save.")
